top_domains_whois <- top_domains_whois %>%
mutate(trust = if_else(domain_name %in% fake, 'fake', 'initial trust' ))
# Add alexa rankings
target <- 'data/input/top-1m.csv'
alexa <- read_csv(target, col_names = c('rank', 'domain_name'), col_types = 'ic')
combined_tbl <- union(top_domains_whois, fake_domains_whois)
combined_tbl <- right_join(alexa, combined_tbl, by = 'domain_name')
# Cleanup duplicates
combined_tbl <- combined_tbl %>% distinct(domain_name, .keep_all = TRUE)
rm(alexa)
combined_tbl$domain_name <- as.factor(combined_tbl$domain_name)
combined_tbl$trust <- as.factor(combined_tbl$trust)
# Let's audit the data with some known disinformation sites
search_domain <- function(df, dom){
df %>%
filter(.data$domain_name == dom) %>%
select(.data$rank, .data$domain_name, .data$trust)
}
# The Epoch Times is connected with the Falun Gong movement in China
search_domain(combined_tbl, 'theepochtimes.com')
# Showing initial trust (fail)
search_domain(combined_tbl, 'breitbart.com')
# Showing initial trust (fail)
search_domain(combined_tbl, 'google.com')
search_domain(combined_tbl, '100percentfedup.com')
dailydot <- read_csv('dailydot.csv') %>% glimpse
dailydot <- read_csv('data/output/dailydot.csv') %>% glimpse
fake_list <- read_csv('data/output/table_scrape.csv') %>% glimpse
fake_list <- read_csv('data/output/table_scrape.csv') %>% select(1)
fake_list <- read_csv('data/output/table_scrape.csv')
fake_list.tail()
fake_list.tail()
fake_list %>% tail
fake_list <- fake_list %>% select(1)
dailydot <- read_csv('data/output/dailydot.csv')
dailydot <- read_csv('data/output/dailydot.csv',col_names = 'domain_name')
dailydot <- read_csv('data/output/dailydot.csv',col_names = 'domain_name')
dailydot <- read_csv('data/output/dailydot.csv')
daiydot %>% pivot
dailydot %>% pivot
library(tidyverse)
dailydot %>% pivot_longer()
dailydot %>% pivot_longer(names_to = 'domain_name')
dailydot %>% pivot_longer(everything())
dailydot %>% pivot_longer(everything(), names_to = 'domain_name')
cols(dailydot)
dailydot <- read_csv('data/output/dailydot.csv')
dailydot <- read_csv('data/output/dailydot.csv',col_names = 'domain_name')
fake_list <- read_csv('data/output/table_scrape.csv')
fake_list <- fake_list %>% select(domain_name = 1)
fake_scrapes <- union(dailydot, fake_list)
rm(dailydot, fake_list)
fake_list <- fake_list %>% select(domain_name = 1)
fake_list <- read_csv('data/output/table_scrape.csv')
fake_list <- fake_list %>% select(domain_name = 1)
fake_scrapes <- union(dailydot, fake_list)
dailydot <- read_csv('data/output/dailydot.csv',col_names = 'domain_name')
fake_list %>% View
fake_list %>% distinct(domain_name)
fake_list <- fake_list %>% distinct(domain_name)
fake_scrapes <- union(dailydot, fake_list)
rm(dailydot, fake_list)
setdiff(combined_tbl$domain_name, fake_scrapes$domain_name)
intersect(combined_tbl$domain_name, fake_scrapes$domain_name)
setdiff(fake_scrapes$domain_name, combined_tbl$domain_name)
intersect
intersect(combined_tbl$domain_name, fake_scrapes$domain_name)
top_domains_whois <- read_csv('data/output/top_domains_whois.csv')
top_domains_whois <- top_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf')
top_domains_whois$domain_name <- str_to_lower(top_domains_whois$domain_name)
top_domains_whois %>% distinct(domain_name)
top_domains_whois <- top_domains_whois %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE)
# rm(top_websites, domains, fake_domains, fake, top_websites) #cleanup
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcff')
fake_domains_whois$trust <- 'fake'
top_domains_whois$trust <- as.factor('initial trust')
write_csv(fake_scrapes, 'fake_scrapes.csv')
write_csv(fake_scrapes, 'data/output/fake_scrapes.csv')
## Begin populating tables
top_domains_whois <- read_csv('data/output/top_domains_whois.csv')
top_domains_whois <- top_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE)
## Begin populating tables
top_domains_whois <- read_csv('data/output/top_domains_whois.csv')
top_domains_whois <- top_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('initial trust'))
## Add the fake domains df
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcff') %>%
mutate(trust = as.factor('initial trust'))
## Add the fake domains df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcff') %>%
mutate(trust = as.factor('initial trust'))
PATH <- 'data/input/'
rm(list = ls())
library(rvest)
library(tidyverse)
PATH <- 'data/input/'
# Kaggle datasets
true <- paste(PATH, 'ATrue-kaggle.csv', sep = '')
false <- paste(PATH, 'AFake-kaggle.csv', sep = '')
read_csv(true) %>% glimpse()
read_csv(false) %>% glimpse()
`%ni%` <- Negate(`%in%`)
fake <- read_csv('data/input/fake-kaggle.csv')
drop_columns <- c('uuid', 'ord_in_thread', 'crawled')
text_split <- function(x) { sapply(str_split(x, ' '), length) }
fake <- fake %>%
select(-!!drop_columns) %>%
type_convert(col_types = 'f?ccfffdfdfdddddf') %>%
token %>%
mutate(text_length = lapply(text_tokens, length) %>% unlist) %>%
mutate(title_length = lapply(title_tokens, length) %>% unlist) %>%
group_by(site_url) %>%
arrange(site_url, published %>% desc)
fake_domains <- fake %>% select(site_url) %>% unique
file.copy('data/input/fake-kaggle.csv', 'data/input/analysis/fake-kaggle.csv')
file.remove('data/input/fake-kaggle.csv')
## FNN Politics Fake
fake <- paste(PATH, 'fnn_politics_fake-kaggle.csv', sep = '')
fake <- read_csv(fake) %>% glimpse
url_match <- '^(?:https?:\\/\\/)?(?:www\\.)?([^\\/\\r\\n]+)(\\/[^\\r\\n]*)?'
web_archive_rm <- '^https:\\/\\/web.archive.org\\/.*(?=http)'
rm(true, talse)
rm(false)
## FNN Politics Fake
fake <- paste(PATH, 'fnn_politics_fake-kaggle.csv', sep = '')
fake <- read_csv(fake) %>% glimpse
url_match <- '^(?:https?:\\/\\/)?(?:www\\.)?([^\\/\\r\\n]+)(\\/[^\\r\\n]*)?'
web_archive_rm <- '^https:\\/\\/web.archive.org\\/.*(?=http)'
fake <- fake %>%
select(-tweet_ids) %>%
# strip http
mutate(news_url = str_remove(news_url, web_archive_rm)) %>%
mutate(site_url = str_replace(news_url, url_match, replacement = '\\1'))
fake_domains <- fake %>%
mutate(site_url = str_remove(site_url, ':\\d+$')) %>%
group_by(site_url) %>%
summarise(site_url) %>%
union(fake_domains)
dir.create('data/input/analysis')
file.copy('data/input/fnn_politics_fake-kaggle.csv', 'data/input/analysis/fnn_politics_fake-kaggle.csv')
file.remove(c('data/input/fnn_politics_fake-kaggle.csv', 'data/input/fnn_politics_real-kaggle.csv'))
rm(drop_columns) #cleanup
fake <- read_csv('data/input/news_articles-kaggle.csv')
file.copy(target, 'data/input/analysis/kaggle-best.csv')
fake <- read_csv('data/input/kaggle-best.csv')
file.copy(target, 'data/input/analysis/kaggle-best.csv')
file.copy('data/input/kaggle-best.csv', 'data/input/analysis/kaggle-best.csv')
file.remove('data/input/kaggle-best.csv')
fake_domains <- fake %>% select(site_url) %>% union(fake_domains)
fake_domains <- fake %>% select(site_url) %>% union(fake_domains)
link <- 'https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vTY6gW8su8k-BxuQo6CkwEDfsQq3TnVdqEOOqtjOeZ6YeYiXyZ-ZjZnN6pDKmQHpfZ9sXhUHmnduKUr/pubhtml?gid=0&single=true'
fake_news <- read_html(link)
fake_news <- fake_news %>%
html_node('table') %>% html_table(header=FALSE) %>% tbl_df
fake_news <- fake_news %>%
select(site_url = 4) %>%
slice(-(1:3))
fake_domains <- union(fake_domains, fake_news)
target <- 'data/input/reopen-krebs.csv'
fake <- readr::read_csv(target)
fake %>% glimpse()
file.copy(target, 'data/input/analysis/reopen-krebs.csv')
file.remove(target)
fake$`DOMAIN NAME`
fake_domains <- union(fake_domains, fake %>% select(site_url = 1))
target <- 'data/input/poynter_covid_claims_data.csv'
readr::read_csv(target)%>% glimpse()
file.remove(target)
## Add in dailydot scrape
fake <- read_csv('data/output/dailydot.csv',col_names = 'domain_name')
fake %>% union(fake_domains)
fake_domains <- select(domain_name = site_url)
fake_domains <- fake_domains %>% select(domain_name = site_url)
fake %>% union(fake_domains)
fake_domains <- fake %>% union(fake_domains)
## Add in table scrape
fake <- read_csv('data/output/table_scrape.csv')
## Add in table scrape
fake <- read_csv('data/output/table_scrape.csv') %>% select(domain_name = 1)
fake_domains <- fake %>% union(fake_domains)
fake_domains %>% distinct(domain_name, .keep_all = TRUE)
fake_domains <- fake_domains %>% distinct(domain_name, .keep_all = TRUE)
fake_domains <- fake_domains %>% distinct(domain_name, .keep_all = TRUE)
## Take file and scrape whois data
write_csv(fake_domains, 'data/output/fake_domains.csv')
rm(fake, fake_news) # cleanup
##
topsites_df <- 'data/input/top-1m.csv'
rm(topsites_df)
##
top_websites <- read_csv('data/input/top-1m.csv',
col_names = c('rank', 'site_url'),
col_types = 'ic')
## Add the top domains df
top_domains_whois <- read_csv('data/output/top_domains_whois.csv')
top_domains_whois <- top_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE)
top_domains_whois$domain_name <- str_to_lower(top_domains_whois$domain_name)
library(tldextract)
tldextract('google.com')
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcff') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('initial trust'),
domain_name = str_to_lower(domain_name))
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('initial trust'),
domain_name = str_to_lower(domain_name))
## Check which domains show in both lists, manually adjust
intersect(top_domains_whois$domain_name, fake_domains_whois$domain_name)
## Check which domains show in both lists, manually adjust
both <- intersect(top_domains_whois$domain_name, fake_domains_whois$domain_name)
both %>% View
fake <- c(0,0,0,0,0,0,0,0,0,0,
0,0,1,0,0,1,1,0,0,0,
0,0,0,0,1,1,0,1,0,0,
0,0,0,1,1,0,0,1,0,0,
1,0,1,0,0,0,0,0,0,1,
1,0,0,0,0,1,0,0,0,1,
0,0,1,1,0,0,0,0,1,0,
1,0,1,0,0,1,0,0,1,1,
1,0,0,0,1)
both[as.logical(fake)]
fake <- c(0,0,0,0,0,0,0,0,0,0,
0,0,1,0,0,1,1,0,0,0,
0,0,0,0,1,1,0,1,0,0,
0,0,0,1,1,0,0,1,0,0,
1,0,1,0,0,0,0,0,0,1,
1,0,0,0,0,1,0,0,0,0,
1,0,1,1,0,0,0,0,1,0,
1,0,1,0,0,1,0,0,1,1,
1,0,0,0,1)
both[as.logical(fake)]
fake <- both[as.logical(fake)]
top_domains_whois %>% filter(domain_name %in% fake)
top_domains_whois <- top_domains_whois %>% filter(domain_name %ni% fake)
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('initial trust'),
domain_name = str_to_lower(domain_name))
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('fake'),
domain_name = str_to_lower(domain_name))
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('fake'),
domain_name = str_to_lower(domain_name))
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(domain_name = str_to_lower(domain_name))
## Check which domains show in both lists, manually adjust
both <- intersect(top_domains_whois$domain_name, fake_domains_whois$domain_name)
both %>% View
fake <- c(0,0,0,0,0,0,0,0,0,0,
0,0,1,0,0,1,1,0,0,0,
0,0,0,0,1,1,0,1,0,0,
0,0,0,1,1,0,0,1,0,0,
1,0,1,0,0,0,0,0,0,1,
1,0,0,0,0,1,0,0,0,0,
1,0,1,1,0,0,0,0,1,0,
1,0,1,0,0,1,0,0,1,1,
1,0,0,0,1)
fake <- both[as.logical(fake)]
# Filter out of the top ones...
top_domains_whois <- top_domains_whois %>% filter(domain_name %ni% fake)
# And add to & distinct in fake_domains
fake_domains_whois <- fake_domains_whois %>%
filter(domain_name %ni% fake)
# And add to & distinct in fake_domains
fake_domains_whois <- fake_domains_whois %>%
filter(domain_name %ni% fake)
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(domain_name = str_to_lower(domain_name))
# And add to & distinct in fake_domains
fake_domains_whois %>%
filter(domain_name %ni% fake)
# And add to & distinct in fake_domains
fake_domains_whois %>%
filter(domain_name %in% fake)
# And add to & distinct in fake_domains
fake_domains_whois %>%
filter(domain_name %ni% fake)
# And add to & distinct in fake_domains
fake_domains_whois %>%
filter(domain_name %ni% fake) %>%
distinct(domain_name, .keep_all = TRUE)
# And add to & distinct in fake_domains
fake_domains_whois <- fake_domains_whois %>%
filter(domain_name %ni% fake) %>%
distinct(domain_name, .keep_all = TRUE)
# Combine datasets and add site ranking
alexa <- read_csv('data/input/top-1m.csv',
col_names = c('rank', 'domain_name'),
col_types = 'ic')
all_web_whois <- union(fake_domains_whois, top_domains_whois)
all_web_whois <- right_join(alexa, all_web_whois, by = 'domain_name')
rm(alexa, fake_domains_whois, top_domains_whois, fake_domains, top_websites)
## Add the top domains whois df
top_domains_whois <- read_csv('data/output/top_domains_whois.csv')
top_domains_whois <- top_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('fake'),
domain_name = str_to_lower(domain_name))
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(domain_name = str_to_lower(domain_name))
## Check which domains show in both lists, manually adjust
both <- intersect(top_domains_whois$domain_name, fake_domains_whois$domain_name)
both %>% View
fake <- c(0,0,0,0,0,0,0,0,0,0,
0,0,1,0,0,1,1,0,0,0,
0,0,0,0,1,1,0,1,0,0,
0,0,0,1,1,0,0,1,0,0,
1,0,1,0,0,0,0,0,0,1,
1,0,0,0,0,1,0,0,0,0,
1,0,1,1,0,0,0,0,1,0,
1,0,1,0,0,1,0,0,1,1,
1,0,0,0,1)
fake <- both[as.logical(fake)]
# Filter out of the top ones...
top_domains_whois <- top_domains_whois %>% filter(domain_name %ni% fake)
# And add to & distinct in fake_domains
fake_domains_whois <- fake_domains_whois %>%
filter(domain_name %ni% fake) %>%
distinct(domain_name, .keep_all = TRUE) %>%
mutate(trust = 'fake')
## Add the top domains whois df
top_domains_whois <- read_csv('data/output/top_domains_whois.csv')
top_domains_whois <- top_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('initial trust'),
domain_name = str_to_lower(domain_name))
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(domain_name = str_to_lower(domain_name))
## Check which domains show in both lists, manually adjust
both <- intersect(top_domains_whois$domain_name, fake_domains_whois$domain_name)
both %>% View
fake <- c(0,0,0,0,0,0,0,0,0,0,
0,0,1,0,0,1,1,0,0,0,
0,0,0,0,1,1,0,1,0,0,
0,0,0,1,1,0,0,1,0,0,
1,0,1,0,0,0,0,0,0,1,
1,0,0,0,0,1,0,0,0,0,
1,0,1,1,0,0,0,0,1,0,
1,0,1,0,0,1,0,0,1,1,
1,0,0,0,1)
fake <- both[as.logical(fake)]
# Filter out of the top ones...
top_domains_whois <- top_domains_whois %>% filter(domain_name %ni% fake)
# And add to & distinct in fake_domains
fake_domains_whois <- fake_domains_whois %>%
filter(domain_name %ni% fake) %>%
distinct(domain_name, .keep_all = TRUE) %>%
mutate(trust = 'fake')
# Combine datasets and add site ranking
alexa <- read_csv('data/input/top-1m.csv',
col_names = c('rank', 'domain_name'),
col_types = 'ic')
all_web_whois <- union(fake_domains_whois, top_domains_whois)
all_web_whois <- right_join(alexa, all_web_whois, by = 'domain_name')
rm(alexa, fake_domains_whois, top_domains_whois, fake_domains, top_websites)
all_web_whois %>% head(50) %>% View
selection <- c(0,0,0,0,0,0,0,0,0,0,
0,0,1,0,0,1,1,0,0,0,
0,0,0,0,1,1,0,1,0,0,
0,0,0,1,1,0,0,1,0,0,
1,0,1,0,0,0,0,0,0,1,
1,0,0,0,0,1,0,0,0,0,
1,0,1,1,0,0,0,0,1,0,
1,0,1,0,0,1,0,0,1,1,
1,0,0,0,1)
fake <- both[as.logical(selection)]
real <- both[!as.logical(selection)]
fake
real
## Add the top domains whois df
top_domains_whois <- read_csv('data/output/top_domains_whois.csv')
top_domains_whois <- top_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(trust = as.factor('initial trust'),
domain_name = str_to_lower(domain_name))
## Add the fake domains whois df
fake_domains_whois <- read_csv('data/output/fake_domains_whois.csv')
fake_domains_whois <- fake_domains_whois %>%
type_convert(col_types = 'fffTTTcccfcfcffcf') %>%
distinct(domain_name, .keep_all = TRUE) %>%
filter(is.na(domain_name) == FALSE) %>%
mutate(domain_name = str_to_lower(domain_name))
## Check which domains show in both lists, manually adjust
both <- intersect(top_domains_whois$domain_name, fake_domains_whois$domain_name)
selection <- c(0,0,0,0,0,0,0,0,0,0,
0,0,1,0,0,1,1,0,0,0,
0,0,0,0,1,1,0,1,0,0,
0,0,0,1,1,0,0,1,0,0,
1,0,1,0,0,0,0,0,0,1,
1,0,0,0,0,1,0,0,0,0,
1,0,1,1,0,0,0,0,1,0,
1,0,1,0,0,1,0,0,1,1,
1,0,0,0,1)
fake <- both[as.logical(selection)]
real <- both[!as.logical(selection)]
# Filter out of the top ones...
top_domains_whois <- top_domains_whois %>% filter(domain_name %ni% fake)
# And add to & distinct in fake_domains
fake_domains_whois <- fake_domains_whois %>%
filter(domain_name %ni% real) %>%
distinct(domain_name, .keep_all = TRUE) %>%
mutate(trust = 'fake')
fake_domains_whois %>% View
top_domains_whois %>% View
# Combine datasets and add site ranking
alexa <- read_csv('data/input/top-1m.csv',
col_names = c('rank', 'domain_name'),
col_types = 'ic')
all_web_whois <- union(fake_domains_whois, top_domains_whois)
all_web_whois %>% View
rm(alexa, fake_domains_whois, top_domains_whois, fake_domains, top_websites)
all_web_whois <- right_join(alexa, all_web_whois, by = 'domain_name')
# Combine datasets and add site ranking
alexa <- read_csv('data/input/top-1m.csv',
col_names = c('rank', 'domain_name'),
col_types = 'ic')
all_web_whois <- union(fake_domains_whois, top_domains_whois)
all_web_whois <- right_join(alexa, all_web_whois, by = 'domain_name')
rm(alexa, fake_domains_whois, top_domains_whois, fake_domains, top_websites)
all_web_whois %>% View
# Let's audit the data with some known disinformation sites
search_domain <- function(df, dom){
df %>%
filter(.data$domain_name == dom) %>%
select(.data$rank, .data$domain_name, .data$trust)
}
# The Epoch Times is connected with the Falun Gong movement in China
search_domain(all_web_whois, 'theepochtimes.com')
# Showing initial trust (fail)
search_domain(all_web_whois, 'breitbart.com')
# Showing initial trust (fail)
search_domain(all_web_whois, 'google.com')
search_domain(all_web_whois, '100percentfedup.com')
# The Epoch Times is connected with the Falun Gong movement in China
search_domain(all_web_whois, '8kun.top')
# Showing initial trust (fail)
search_domain(all_web_whois, 'breitbart.com')
# Looks OK so far, though will need some correction
colnames(all_web_whois)
all_web_whois %>%
select(-(9:10))
all_web_whois %>%
select(-c(9, 10, 12, 14))
all_web_whois %>%
select(-c(9, 10, 12, 14))
all_web_whois <- all_web_whois %>% select(-c(9, 10, 12, 14))
ga_query_doms <- all_web_whois %>%
filter(trust == 'fake')
write_csv(ga_query_doms, 'data/output/ga_query_doms.csv')
words
