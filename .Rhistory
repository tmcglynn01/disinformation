getwd()
setwd('C:/rstudio/')
library(tidyverse)
library(lubridate)
?diamonds
df <- diamonds
glimpse(diamonds)
diamonds[1,1]
diamonds[1]
diamonds[,1]
diamonds[[,1]]
diamonds[[1,1]]
diamonds[[1,]]
diamonds[[1]]
diamonds
df[1]
df[1,1]
df[,1]
df[1,]
df[[1,]]
df[1]
df[1,]
glimpse(df)
average(df$carat)
df %>% mean(carat)
mean(df$carat)
median(df$carat)
summary(df$carat)
summary(df)
sum(df$price)
nrows(df)
nrow(df)
max(df$price)
min(df$price)
range(df$price)
names(knitr::knit_engines$get())
install.packages('reticulate')
Y
}
library(tidyverse)
?read_csv
vignette('readr')
vignette('lubridate')
?mdy
reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant))
rm(df)
library(tidyverse)
library(lubridate)
setwd('C:/github/disinformation/')
dir.create('data\\output', showWarnings = FALSE)
## This script is used to wrangle/munge data associated with disinformation.
fakeset_a <- read_csv('data/AFake-kaggle.csv', col_types = 'ccfc') %>% glimpse
fka_clean <- fakeset_a %>%
mutate(date = parse_date_time(date, orders = c('mdy', 'dmy')),
title_length = str_length(title),
text_length = sapply(strsplit(text, ' '), length)) %>%
arrange(date, subject)
realset_a <- read_csv('data/ATrue-kaggle.csv', col_types = 'ccfc')
rla_clean <- realset_a %>%
mutate(date = parse_date_time(date, orders = c('mdy', 'dmy')),
title_length = str_length(title),
text_length = sapply(strsplit(text, ' '), length)) %>%
arrange(date, subject)
rm(fakeset_a, realset_a)
## FIXME: ADD DATA DESCRIPTION
fakeset_b <- read_csv('data/fake-kaggle.csv')
drop_columns <- c('uuid', 'crawled')
fkb_clean <- fakeset_b %>%
select(-!!drop_columns) %>%
arrange(published, country, language) %>%
separate(site_url, c('domain_name', 'tld')) %>%
mutate(author = parse_factor(author),
language = parse_factor(language),
domain_name = parse_factor(domain_name),
tld = parse_factor(tld),
country = parse_factor(country),
type = parse_factor(type),
main_img_domain = str_extract(main_img_url, '\\w+\\.\\w{2,}(?=/)'),
title_length = str_length(title),
text_length = sapply(strsplit(text, ' '), length)) %>%
arrange(published, country, language)
rm(fakeset_b)
reopen_domains <- read_csv('data\\reopen-krebs.csv') %>% glimpse
reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant))
parse_date_time('2020-02-01 12:24:34' '%F %T')
parse_date_time('2020-02-01 12:24:34', '%F %T')
parse_date_time('2020-02-01 12:24:34', '%c')
parse_date_time('2020-02-01 12:24:34', 'ymd hms')
parse_date_time('2020-02-01 12:24:34', 'ymd HMS')
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>%
glimpse
reopen_domains <- read_csv('data\\reopen-krebs.csv')
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
rename(active = `ACTIVE?`) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>%
glimpse
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
rename(`ACTIVE?` = active) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>%
glimpse
reopen_domains <- read_csv('data/reopen-krebs.csv')
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
rename(active = `ACTIVE?`) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>%
glimpse
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>%
glimpse
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>%
glimpse
reopen_domains <- read_csv('data/reopen-krebs.csv')
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
rename(active = `active?`) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>%
glimpse
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
rename(active = `active?`) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp)
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
rename(active = `active?`) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp) %>% head(n = 25)
reopen_clean <- reopen_domains %>%
select_all(~str_replace(., ' ', '_')) %>%
select_all(tolower) %>%
rename(active = `active?`) %>%
separate(domain_name, c('domain', 'tld')) %>%
mutate(date = parse_date_time(date, c('mdy', '%m-%d%y')),
registrar = parse_factor(registrar),
registrant = parse_factor(registrant),
timestamp = parse_date_time(timestamp, 'ymd HMS'),
state = recode_factor(state,
'NEV' = 'NV',
'FLA' = 'FL',
'WIS' = 'WI',
'KAN' = 'KS',
'ALA' = 'AL')) %>%
arrange(date, state, timestamp)
reopen_clean %>% glimpse
unique(reopen_clean$notes)
un_reopen_notes <- unique(reopen_clean$notes)
map(un_reopen_notes, str_extract(., '\\.+[@]\\.+\\..+'))
str_extract(., '\\.+[@]\\.+\\..+')
str_extract(un_reopen_notes, '\\.+[@]\\.+\\..+')
str_extract(un_reopen_notes, '\\S+@\\S+\\.\\S+')
domain_extract <- '\\w+\\.\\w{2,}(?=/)'
extract_emails = function(target) {
re_pattern <- '\\.+[@]\\.+\\..+'
str_extract(target, re_pattern)
}
extract_emails(un_reopen_notes)
str_extract(un_reopen_notes, '\\.+[@]\\.+\\..+')
un_reopen_notes <- unique(reopen_clean$notes)
str_extract(un_reopen_notes, '\\.+[@]\\.+\\..+')
str_extract(un_reopen_notes, '\\S+@\\S+\\.\\S+')
reopen_clean %>%
str_extract(notes, '\\S+@\\S+\\.\\S+')
reopen_clean %>%
mutate(notes_emails = str_extract(notes, '\\S+@\\S+\\.\\S+'))
reopen_clean %>%
select(notes) %>%
mutate(notes_emails = str_extract(notes, '\\S+@\\S+\\.\\S+'))
un_reopen_notes
reopen_clean %>%
select(notes) %>%
mutate(notes_emails = str_extract(notes, '\\S+@\\S+\\.\\S+'),
notes_ga = str_extract(notes, 'UA-\\d+-\\d+'))
reopen_clean %>%
select(notes) %>%
mutate(notes_emails = str_extract(notes, '\\S+@\\S+\\.\\S+'),
notes_ga = str_extract(notes, 'UA-\\d+-\\d+')) %>%
drop_na(notes_ga)
# Adds email and Google Analytics ID columns where detected
reopen_clean <- reopen_clean %>%
select(notes) %>%
mutate(notes_emails = str_extract(notes, '\\S+@\\S+\\.\\S+'),
notes_ga = str_extract(notes, 'UA-\\d+-\\d+'))
news_articles <- read_csv('data\\news_articles-kaggle.csv') %>% glimpse
rm(reopen_domains)
vignette('readr')
news_articles <- read_csv('data\\news_articles-kaggle.csv',
col_types = 'fTccfcccffccl') %>% glimpse
news_articles <- read_csv('data\\news_articles-kaggle.csv',
col_types = 'fTccfcccffcl') %>% glimpse
news_articles %>%
mutate(title_length = sapply(str_split(title, ' '), length),
text_length = sapply(str_split(text, ' ', length))) %>%
separate(site_url, c('domain', 'tld')) %>%
mutate(img_domain = str_extract(main_img_url, domain_extract),
type = parse_factor(type))
news_articles %>%
mutate(title_length = sapply(str_split(title, ' '), length),
text_length = sapply(str_split(text, ' '), length)) %>%
separate(site_url, c('domain', 'tld')) %>%
mutate(img_domain = str_extract(main_img_url, domain_extract),
type = parse_factor(type))
